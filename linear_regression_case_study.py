# -*- coding: utf-8 -*-
"""Linear regression case study.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12-6PHUmSbLDvsPD-bS06cAWhk0xhnQJb
"""

!gdown https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/001/839/original/Jamboree_Admission.csv

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")

"""**Definition of problem**

Helping Jamboree in providing best model to predict one's chances of admission given the given features by understanding of what factors are important in graduate admissions and how these factors are interrelated among themselves.

**Data preprocessing and Exploratory Data analysis**
"""

df = pd.read_csv('Jamboree_Admission.csv')

df.head(10)

df.info()  #information about a DataFrame including the index dtype and columns, non-null values and memory usage.

df.size,df.shape   #provide shape and size of the dataset

df.drop(columns='Serial No.',inplace = True) # Drop the irrelevant data like unique row Identifier and others, if any

df.isnull().sum()   #checking if there are any null values in the dataset

"""**Distribution of the variables of graduate applicants -Univariate Analysis**"""

df.head()

# @title GRE Score

from matplotlib import pyplot as plt
df['GRE Score'].plot(kind='hist', bins=20, title='GRE Score')
plt.gca().spines[['top', 'right',]].set_visible(False)

# @title TOEFL Score

from matplotlib import pyplot as plt
df['TOEFL Score'].plot(kind='hist', bins=20, title='TOEFL Score')
plt.gca().spines[['top', 'right',]].set_visible(False)

#@title University Rating
from matplotlib import pyplot as plt
df['University Rating'].plot(kind='hist', bins=20, title='University Rating')
plt.gca().spines[['top', 'right',]].set_visible(False)

## @title CGPA
from matplotlib import pyplot as plt
df['CGPA'].plot(kind='hist', bins=20, title='CGPA')
plt.gca().spines[['top', 'right',]].set_visible(False)

# @title Chance of Admission
from matplotlib import pyplot as plt
df['Chance of Admit '].plot(kind='hist', bins=20, title='Chance of Admit ')
plt.gca().spines[['top', 'right',]].set_visible(False)

"""***Observation: from the univariant analysis,It is clear from the distributions, students with varied merit apply for the university***

**Relationships between important features for 'chance of admission'**
"""

# @title GRE Score vs TOEFL Score
fig = sns.regplot(x="GRE Score", y="TOEFL Score", data=df)
plt.title("GRE Score vs TOEFL Score")
plt.show()

"""Both TOEFL and GRE have a verbal section.Applicants with higher GRE Scores also have higher TOEFL Scores which is relatable"""

# @title GRE Score vs CGPA
fig = sns.regplot(x="GRE Score", y="CGPA", data=df)
plt.title("GRE Score vs CGPA")
plt.show()

"""eventhough there are exceptions, people with higher CGPA usually have higher GRE scores, high university ratings"""

# @title University Rating vs CGPA
import matplotlib.pyplot as plt
plt.scatter(df['University Rating'], df['CGPA'], c=df['CGPA'])
plt.xlabel('University Rating')
_ = plt.ylabel('CGPA')

# @title University Rating vs GRE score
import matplotlib.pyplot as plt
plt.scatter(df['University Rating'], df['GRE Score'], c=df['GRE Score'])
plt.xlabel('University Rating')
_ = plt.ylabel('GRE Score')

fig = sns.scatterplot(x="CGPA", y="LOR ", data=df, hue="Research")
plt.title("LOR vs CGPA")
plt.show()

"""LORs are not that related with CGPA so it is clear that a persons LOR is not dependent on
that persons academic excellence. Having research experience is usually related with a
good LOR which might be justified by the fact that supervisors have personal interaction
with the students performing research which usually results in good LORs

"""

fig = sns.scatterplot(x="GRE Score", y="LOR ", data=df, hue="Research")
plt.title("GRE Score vs LOR")
plt.show()

"""GRE scores and LORs are also not that related. People with different kinds of LORs have all
kinds of GRE scores

"""

fig = sns.scatterplot(x="CGPA", y="SOP", data=df)
plt.title("SOP vs CGPA")
plt.show()

"""CGPA and SOP are not that related because Statement of Purpose is related to academic
performance, but since people with good CGPA tend to be more hard working so they have
good things to say in their SOP which might explain the slight move towards higher CGPA
as along with good SOPs
"""

fig = sns.scatterplot(x="TOEFL Score", y="SOP", data=df)
plt.title("TOEFL Score vs SOP")
plt.show()

"""Applicants with different kinds of SOP have different kinds of TOEFL Score. So the quality of
SOP is not always related to the applicants English skills.

**corelation between the features of the given data **
"""

corr = df.corr()

corr = pd.DataFrame(corr)
plt.figure(figsize=(10, 8))
sns.heatmap(corr, annot=True, cmap="coolwarm")
plt.show()

"""**Model building**

split the dataset with training and testing set and prepare the inputs and outputs
"""

from sklearn.model_selection import train_test_split  #Dropping labelled/dependent feature
X = df.drop(['Chance of Admit '], axis=1)
y = df['Chance of Admit ']

df.head()

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=1)

X_train

y_train

"""**Standardization/scaling**"""

from sklearn.preprocessing import  StandardScaler

X_train_columns=X_train.columns
std=StandardScaler()
X_train_std=std.fit_transform(X_train)

X_train_std

X_train=pd.DataFrame(X_train_std, columns=X_train_columns)

X_train

"""**Lets use a bunch of different algorithms to see which model performs better**"""

from sklearn.metrics import accuracy_score
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Lasso,Ridge,LinearRegression
from sklearn.metrics import mean_squared_error

models = [
 ['Linear Regression :', LinearRegression()],

 ['Lasso Regression :', Lasso(alpha=0.1)], #try with differen
 ['Ridge Regression :', Ridge(alpha=1.0)] #try with different
 ]

print("Results without removing features with multicollinearity ...")
for name,model in models:
  model.fit(X_train, y_train.values)
  predictions = model.predict(std.transform(X_test))
  print(name, (np.sqrt(mean_squared_error(y_test, predictions))))

"""**Linear Regression using Statsmodel library**
->Adjusted R-squared reflects the fit of the model. R-squared values range from
0 to 1,where a higher value generally indicates a better fit, assuming certain conditions are met.

->const coefficient is your Y-intercept. It means that if both the Interest_Rate and Unemployment_Rate coefficients are zero, then the expected output (i.e., the Y) would be equal to the const coefficient.

->Interest_Rate coefficient represents the change in the output Y due to a change of one unit in the interest rate (everything else held constant)

->Unemployment_Rate coefficient represents the change in the output Y due to a change of one unit in the unemployment rate (everything else held constant)

->std err reflects the level of accuracy of the coefficients. The lower it is, the higher is the level of accuracy

->P >|t| is your p-value. A p-value of less than 0.05 is considered to be statistically significant

->Confidence Interval represents the range in which our coefficients are likely to fall (with a likelihood of 95%)


"""

import statsmodels.api as sm
X_train = sm.add_constant(X_train)
model = sm.OLS(y_train.values, X_train).fit()
print(model.summary())

X_train_new=X_train.drop(columns='SOP')

model1 = sm.OLS(y_train.values, X_train_new).fit()
print(model1.summary())

"""**VIF(Variance Inflation Factor)**"""

from statsmodels.stats.outliers_influence import variance_inflation_factor
def calculate_vif(dataset,col):
  dataset=dataset.drop(columns=col,axis=1)
  vif=pd.DataFrame()
  vif['features']=dataset.columns
  vif['VIF_Value']=[variance_inflation_factor(dataset.values,i) for i in range(dataset.shape[1])]
  return vif

calculate_vif(X_train_new,[])

"""VIF looks fine and hence, we can go ahead with the predictions

"""

X_test_std= std.transform(X_test)
X_test=pd.DataFrame(X_test_std, columns=X_train_columns) # col name sa

X_test = sm.add_constant(X_test)
X_test_del=list(set(X_test.columns).difference(set(X_train_new.columns)))
print(f'Dropping {X_test_del} from test set')

X_test_new=X_test.drop(columns=X_test_del)

#Prediction from the clean model
pred = model1.predict(X_test_new)
from sklearn.metrics import mean_squared_error,r2_score,mean_absolute_error
print('Mean Absolute Error ', mean_absolute_error(y_test.values,pred))
print('Root Mean Square Error ', np.sqrt(mean_squared_error(y_test.values,pred)))

"""**Mean of Residuals**"""

residuals = y_test.values-pred
mean_residuals = np.mean(residuals)
print("Mean of Residuals {}".format(mean_residuals))

"""**Test for Homoscedasticity**

"""

p = sns.scatterplot(x=pred,y=residuals)
plt.xlabel('predicted values')
plt.ylabel('Residuals')
plt.ylim(-0.4,0.4)
plt.xlim(0,1)
p = sns.lineplot(x = [0,16],y = [0,0],color='black')
p = plt.title('Residuals vs fitted values plot for homoscedasticity check')

import statsmodels.stats.api as sms
from statsmodels.compat import lzip
name = ['F statistic', 'p-value']
test = sms.het_goldfeldquandt(residuals, X_test)
lzip(name, test)

"""Here null hypothesis is - error terms are homoscedastic and since p-values >0.05,
we fail to reject the null hypothesis

**Normality of residuals**
"""

p = sns.distplot(residuals,kde=True)
p = plt.title('Normality of error terms/residuals')

# Plotting y_test and y_pred to understand the spread.
fig = plt.figure()
plt.scatter(y_test.values, pred)
fig.suptitle('y_test vs y_pred', fontsize=20) # Plot head
plt.xlabel('y_test', fontsize=18) # X-label
plt.ylabel('y_pred', fontsize=16)

"""***Actionable Insights & Recommendations ***

There are more errors when the chances of admit are less, how to fix
- train my model more on the data points where the chances of admit ar
- Feature enginering to highlight or strengthen the datapoints where c
-We can use a complex model that can identify the pattern

**Is this good? we are seeing a pattern?**

**Bias-Variance Tradeoff**
Bias is as a result of over simplified model assumptions.

Variance occurs when the assumptions are too complex.

The more preferred model is one with low bias and low varinace.

Dimensionality reduction and feature selection can decrease variance by simplifying models.

Similarly, a larger training set tends to decrease variance.
For reducing Bias: Change the model, Ensure the date is truly representative(Ensure that the training data is diverse and represents all possible groups or outcomes.), Parameter tuning.

The bias–variance decomposition forms the conceptual basis for regression
regularization methods such as Lasso and ridge regression.

Regularization methods introduce bias into the regression solution that can reduce variance considerably relative to the ordinary least squares (OLS) solution.

Although the OLS solution provides non-biased regression estimates, the lower variance

solutions produced by regularization techniques provide superior MSE performance.

Linear and Generalized linear models can be regularized to decrease their variance at the cost of increasing their bias.
"""